{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import langid\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# Download the NLTK data for part-of-speech tagging\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed5ac5",
   "metadata": {},
   "source": [
    "### Helper Functions for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_tokens(text):\n",
    "    # Split the text into tokens\n",
    "    #tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    # Identify the language of each token\n",
    "    english_tokens = [word for word in text.split() if langid.classify(word)[0] == 'en']\n",
    "\n",
    "    # Reconstruct the text with English tokens\n",
    "    filtered_text = ' '.join(english_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_proper_nouns(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Remove proper nouns (NNP: singular proper noun, NNPS: plural proper noun)\n",
    "    filtered_words = [word for word, pos in pos_tags if pos not in ['NNP', 'NNPS']]\n",
    "\n",
    "    # Reconstruct the text without proper nouns\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9910f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails_and_hyperlinks(text):\n",
    "    # Remove emails\n",
    "    text_no_emails = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove hyperlinks\n",
    "    text_no_links = re.sub(r'http[s]?\\S+', '', text_no_emails,flags=re.IGNORECASE)\n",
    "\n",
    "    return text_no_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db13c18",
   "metadata": {},
   "source": [
    "### Extract Tags from Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"capstone_data.csv\")\n",
    "df=df[df['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_content\"]=df[\"content\"].apply(remove_non_english_tokens)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_emails_and_hyperlinks)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_proper_nouns)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adb2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyBERT(model=\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Function to extract keywords (tags) for each text\n",
    "def extract_tags(text, top_n=5,keyprob=False):\n",
    "    keywords = model.extract_keywords(text,top_n=top_n)\n",
    "    return [keyword[0] for keyword in keywords if keyword[1]>=0.1]\n",
    "\n",
    "# Apply the extract_tags function to each text in the DataFrame\n",
    "df[\"tags_keybert\"] = df[\"clean_content\"].apply(lambda text: extract_tags(text, top_n=20,keyprob=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"keybert_thresh_onlyen_0.1_nltk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c269c",
   "metadata": {},
   "source": [
    "### Document Clusters from Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c44440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"capstone_data.csv\")\n",
    "df=df[df['lang']=='en']\n",
    "df_key=pd.read_csv(\"keybert_thresh_onlyen_0.1_nltk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(df_key,left_on=\"id\",right_on=\"id\")\n",
    "df=df[[\"id\",\"content_x\",\"tags_x\",\"tags_keybert\",\"lang_x\"]]\n",
    "df=df.rename(columns={\"content_x\": \"content\", \"tags_x\": \"tags\",\"lang_x\":\"lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_content\"]=df[\"content\"].apply(remove_non_english_tokens)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_emails_and_hyperlinks)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_proper_nouns)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract BERT embeddings for a document\n",
    "def extract_bert_embeddings(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Extract BERT embeddings for each document\n",
    "df['embeddings'] = df['clean_content'].apply(extract_bert_embeddings)\n",
    "\n",
    "# Compute the linkage matrix for agglomerative clustering\n",
    "embeddings = df['embeddings'].to_list()\n",
    "\n",
    "# Apply agglomerative clustering with a chosen number of clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "linkage_matrix = linkage(embeddings, method='complete', metric ='cosine')\n",
    "dendrogram(linkage_matrix)\n",
    "plt.show()\n",
    "n_clusters = 800\n",
    "distances = linkage_matrix[:, 2]\n",
    "distance_threshold = distances[-(n_clusters - 1)]\n",
    "print(\"Estimated Distance Threshold:\", distance_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ac508",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=None, linkage='complete', affinity='cosine',distance_threshold=0.20361340364626146)\n",
    "df['cluster_label'] = agg_clustering.fit_predict(embeddings)\n",
    "\n",
    "# Print the DataFrame with cluster labels\n",
    "#print(df)\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "# 750 -- 0.20361340364626146\n",
    "# 500 -- 0.24620008010549643\n",
    "# 650 -- 0.21801996529283096\n",
    "# 800 -- 0.1958287028886031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clus=df[['id','cluster_label','tags','tags_keybert','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057edbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clus.to_csv(\"full_text_clustering_id_750.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf45d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display extracted tags for each cluster\n",
    "\n",
    "unique_clusters = df[\"cluster_label\"].unique()\n",
    "for cluster_label in unique_clusters:\n",
    "    cluster_tags=[]\n",
    "    cluster_documents = df[df[\"cluster_label\"] == cluster_label][\"tags_keybert\"].tolist()\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    for i, document in enumerate(cluster_documents):\n",
    "        #print(f\"{i + 1}. {document}\")\n",
    "        my_list = ast.literal_eval(document)\n",
    "        #print(my_list)\n",
    "        tags=[]\n",
    "        for i in my_list:\n",
    "            tags.append(i)\n",
    "        #cluster_tags.append(tags)\n",
    "        for k in tags:\n",
    "            cluster_tags.append(k)\n",
    "    print(cluster_tags)\n",
    "    if len(cluster_tags)!=0:\n",
    "        counter = Counter(cluster_tags)\n",
    "\n",
    "        # Find the top 5 maximum frequency elements\n",
    "        top_5 = counter.most_common(5)\n",
    "    \n",
    "        #print(f\"Cluster {cluster_label}:\")\n",
    "        for i,tag in enumerate(top_5):\n",
    "            print(f\"{i + 1}. {tag}\")\n",
    "        print(\"\\n\")  \n",
    "            #for j in k:\n",
    "                \n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a86834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display human given tags for each cluster\n",
    "unique_clusters = df[\"cluster_label\"].unique()\n",
    "for cluster_label in unique_clusters:\n",
    "    cluster_tags=[]\n",
    "    cluster_documents = df[df[\"cluster_label\"] == cluster_label][\"tags\"].tolist()\n",
    "    for i,document in enumerate(cluster_documents):\n",
    "        my_list = ast.literal_eval(document)\n",
    "        #print(my_list)\n",
    "        tags=[]\n",
    "        for j in my_list:\n",
    "            tags.append(j)\n",
    "        #cluster_tags.append(tags)\n",
    "        for k in tags:\n",
    "            cluster_tags.append(k)\n",
    "    print(cluster_tags)\n",
    "    if len(cluster_tags)!=0:\n",
    "        counter = Counter(cluster_tags)\n",
    "\n",
    "        # Find the top 5 maximum frequency elements\n",
    "        top_5 = counter.most_common(5)\n",
    "    \n",
    "        print(f\"Cluster {cluster_label}:\")\n",
    "        for i,tag in enumerate(top_5):\n",
    "            print(f\"{i + 1}. {tag}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803067d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display document text for each cluster\n",
    "unique_clusters = df[\"cluster_label\"].unique()\n",
    "for cluster_label in unique_clusters:\n",
    "    cluster_documents = df[df[\"cluster_label\"] == cluster_label][\"content\"].tolist()\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    for i, document in enumerate(cluster_documents):\n",
    "        print(f\"{i + 1}. {document}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
