{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e29e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34de0fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import networkx as nx\n",
    "import json\n",
    "import langid\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# Download the NLTK data for part-of-speech tagging\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85432991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19dfe1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1018f",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860086d",
   "metadata": {},
   "source": [
    "## Creating CSV from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f547214",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file=pd.read_json('sm_pads.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd901130",
   "metadata": {},
   "outputs": [],
   "source": [
    "pads_list = json_file['pads']\n",
    "flattened_data = []\n",
    "\n",
    "for pad in pads_list:\n",
    "    flattened_pad = {\n",
    "        'id': pad['id'],\n",
    "        'content': pad['content'],\n",
    "        'is_public': pad['is_public'],\n",
    "        'tags': '; '.join(pad['tags']),  # Join the list of tags into a string\n",
    "        'sdgs': '; '.join(pad['sdgs']),  # Join the list of sdgs into a string\n",
    "        'country': pad['country'],\n",
    "        'lang': pad['lang']\n",
    "    }\n",
    "    flattened_data.append(flattened_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c267ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>is_public</th>\n",
       "      <th>tags</th>\n",
       "      <th>sdgs</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4364</td>\n",
       "      <td>Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...</td>\n",
       "      <td>True</td>\n",
       "      <td>clean energy; affordable energy; stove</td>\n",
       "      <td>Good health and well-being; Affordable and cle...</td>\n",
       "      <td>ETH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4571</td>\n",
       "      <td>Alokito hridoy - enlightening the souls of chi...</td>\n",
       "      <td>True</td>\n",
       "      <td>education</td>\n",
       "      <td>Quality education</td>\n",
       "      <td>BGD</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6099</td>\n",
       "      <td>BioFabricating Materials\\nSolution holder: Bat...</td>\n",
       "      <td>False</td>\n",
       "      <td>nature-based solutions; design; sustainable fa...</td>\n",
       "      <td>Industry, innovation and infrastructure; Respo...</td>\n",
       "      <td>JOR</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6100</td>\n",
       "      <td>Circulamic\\nSolution holder: Batoul Al-Rashdan...</td>\n",
       "      <td>False</td>\n",
       "      <td>nature-based solutions; recycling; design; sus...</td>\n",
       "      <td>Industry, innovation and infrastructure; Respo...</td>\n",
       "      <td>JOR</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4145</td>\n",
       "      <td>Gakyid Ride (City Bus ride app)\\nThis app prov...</td>\n",
       "      <td>False</td>\n",
       "      <td>digital platform; innovation challenges; innov...</td>\n",
       "      <td>Good health and well-being; Sustainable cities...</td>\n",
       "      <td>BTN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                            content  is_public  \\\n",
       "0  4364  Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...       True   \n",
       "1  4571  Alokito hridoy - enlightening the souls of chi...       True   \n",
       "2  6099  BioFabricating Materials\\nSolution holder: Bat...      False   \n",
       "3  6100  Circulamic\\nSolution holder: Batoul Al-Rashdan...      False   \n",
       "4  4145  Gakyid Ride (City Bus ride app)\\nThis app prov...      False   \n",
       "\n",
       "                                                tags  \\\n",
       "0             clean energy; affordable energy; stove   \n",
       "1                                          education   \n",
       "2  nature-based solutions; design; sustainable fa...   \n",
       "3  nature-based solutions; recycling; design; sus...   \n",
       "4  digital platform; innovation challenges; innov...   \n",
       "\n",
       "                                                sdgs country lang  \n",
       "0  Good health and well-being; Affordable and cle...     ETH   en  \n",
       "1                                  Quality education     BGD   en  \n",
       "2  Industry, innovation and infrastructure; Respo...     JOR   en  \n",
       "3  Industry, innovation and infrastructure; Respo...     JOR   en  \n",
       "4  Good health and well-being; Sustainable cities...     BTN   en  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(flattened_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98959bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "df['tags'] = df['tags'].str.split('; ')\n",
    "df_explode=df.explode('tags')\n",
    "tag_distribution = df_explode['tags'].value_counts()\n",
    "#df['tags'] = df['tags'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6959b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_explode = df.explode('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05624dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean energy', 'affordable energy', 'stove']\n"
     ]
    }
   ],
   "source": [
    "print(df['tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff82686a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>is_public</th>\n",
       "      <th>tags</th>\n",
       "      <th>sdgs</th>\n",
       "      <th>country</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4364</td>\n",
       "      <td>Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...</td>\n",
       "      <td>True</td>\n",
       "      <td>clean energy</td>\n",
       "      <td>Good health and well-being; Affordable and cle...</td>\n",
       "      <td>ETH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4364</td>\n",
       "      <td>Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...</td>\n",
       "      <td>True</td>\n",
       "      <td>affordable energy</td>\n",
       "      <td>Good health and well-being; Affordable and cle...</td>\n",
       "      <td>ETH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4364</td>\n",
       "      <td>Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...</td>\n",
       "      <td>True</td>\n",
       "      <td>stove</td>\n",
       "      <td>Good health and well-being; Affordable and cle...</td>\n",
       "      <td>ETH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4571</td>\n",
       "      <td>Alokito hridoy - enlightening the souls of chi...</td>\n",
       "      <td>True</td>\n",
       "      <td>education</td>\n",
       "      <td>Quality education</td>\n",
       "      <td>BGD</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6099</td>\n",
       "      <td>BioFabricating Materials\\nSolution holder: Bat...</td>\n",
       "      <td>False</td>\n",
       "      <td>nature-based solutions</td>\n",
       "      <td>Industry, innovation and infrastructure; Respo...</td>\n",
       "      <td>JOR</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                            content  is_public  \\\n",
       "0  4364  Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...       True   \n",
       "0  4364  Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...       True   \n",
       "0  4364  Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...       True   \n",
       "1  4571  Alokito hridoy - enlightening the souls of chi...       True   \n",
       "2  6099  BioFabricating Materials\\nSolution holder: Bat...      False   \n",
       "\n",
       "                     tags                                               sdgs  \\\n",
       "0            clean energy  Good health and well-being; Affordable and cle...   \n",
       "0       affordable energy  Good health and well-being; Affordable and cle...   \n",
       "0                   stove  Good health and well-being; Affordable and cle...   \n",
       "1               education                                  Quality education   \n",
       "2  nature-based solutions  Industry, innovation and infrastructure; Respo...   \n",
       "\n",
       "  country lang  \n",
       "0     ETH   en  \n",
       "0     ETH   en  \n",
       "0     ETH   en  \n",
       "1     BGD   en  \n",
       "2     JOR   en  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_explode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c5611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_id                                                    0\n",
      "id                                                        4364\n",
      "content      Tikikil stove\\n\\nGIZ\\nOffice contact GIZ Offic...\n",
      "is_public                                                 True\n",
      "tags                                              clean energy\n",
      "sdgs         Good health and well-being; Affordable and cle...\n",
      "country                                                    ETH\n",
      "lang                                                        en\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_explode['unique_id'] = range(len(df_explode))\n",
    "df_explode = df_explode[['unique_id'] + [col for col in df_explode.columns if col != 'unique_id']]\n",
    "print(df_explode[df_explode['tags'] == 'clean energy'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97979bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"capstone_data_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b74da",
   "metadata": {},
   "source": [
    "## Extract Tags from the Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55767f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_tokens(text):\n",
    "    '''Remove all non english tokens'''\n",
    "    # Split the text into tokens\n",
    "\n",
    "    # Identify the language of each token\n",
    "    english_tokens = [word for word in text.split() if langid.classify(word)[0] == 'en']\n",
    "\n",
    "    # Reconstruct the text with English tokens\n",
    "    filtered_text = ' '.join(english_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453c51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_proper_nouns(text):\n",
    "    '''remove proper nouns such as names of persons'''\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Remove proper nouns (NNP: singular proper noun, NNPS: plural proper noun)\n",
    "    filtered_words = [word for word, pos in pos_tags if pos not in ['NNP', 'NNPS']]\n",
    "\n",
    "    # Reconstruct the text without proper nouns\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db696124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails_and_hyperlinks(text):\n",
    "    '''remove emails and hyperlinks'''\n",
    "    # Remove emails\n",
    "    text_no_emails = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Remove hyperlinks\n",
    "    \n",
    "    text_no_links = re.sub(r'http[s]?\\S+', '', text_no_emails,flags=re.IGNORECASE)\n",
    "\n",
    "    return text_no_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7558ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f46b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"capstone_data_final.csv\")\n",
    "df=df[df['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_content\"]=df[\"content\"].apply(remove_non_english_tokens)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_emails_and_hyperlinks)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_proper_nouns)\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c56746",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyBERT(model=\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Function to extract keywords (tags) for each text\n",
    "def extract_tags(text, top_n=5,keyprob=False):\n",
    "    '''extract top keywords based on the parameter given by user'''\n",
    "    keywords = model.extract_keywords(text,top_n=top_n)\n",
    "    return [keyword[0] for keyword in keywords if keyword[1]>=0.1]\n",
    "\n",
    "# Apply the extract_tags function to each text in the DataFrame\n",
    "df[\"tags_keybert\"] = df[\"clean_content\"].apply(lambda text: extract_tags(text, top_n=20,keyprob=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"keybert_thresh_onlyen_0.1_nltk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2e2a1",
   "metadata": {},
   "source": [
    "##  Semantic clustering of the tags (user assigned + KeyBERT) using SBERT + Root word generation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_creation(combined_df):\n",
    "    combined_df_explode = combined_df.explode('combined_tags')\n",
    "    combined_df_explode['unique_id'] = range(len(combined_df_explode))\n",
    "    combined_df_explode = combined_df_explode[['unique_id'] + [col for col in combined_df_explode.columns if col != 'unique_id']]\n",
    "    combined_df_explode.set_index('unique_id', inplace=True)\n",
    "    combined_df_explode = combined_df_explode.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    combined_corpus = combined_df_explode['combined_tags'].unique()\n",
    "    combined_corpus = list(combined_corpus)\n",
    "    combined_corpus_embeddings = embedder.encode(combined_corpus, show_progress_bar=True)\n",
    "    combined_corpus_embeddings = combined_corpus_embeddings /  np.linalg.norm(combined_corpus_embeddings, axis=1, keepdims=True)\n",
    "    combined_clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold = 0.75, metric='cosine', linkage='average')\n",
    "    combined_clustering_model.fit(combined_corpus_embeddings)\n",
    "    combined_cluster_assignment = combined_clustering_model.labels_\n",
    "    combined_clustered_sentences = {}\n",
    "    for sentence_id, cluster_id in enumerate(combined_cluster_assignment):\n",
    "        if cluster_id not in combined_clustered_sentences:\n",
    "            combined_clustered_sentences[cluster_id] = []\n",
    "\n",
    "        combined_clustered_sentences[cluster_id].append(combined_corpus[sentence_id])\n",
    "    #print(combined_clustered_sentences)\n",
    "    sbert_clusters = {key: [str(value).strip(\"'\") for value in values] for key, values in combined_clustered_sentences.items()}\n",
    "    return sbert_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "def central_word_cossim(row):\n",
    "    # Calculate aggregated vector\n",
    "    #vectors = [nlp(word).vector for word in words]\n",
    "    \n",
    "    ####Normalize first before mean, then cos_sim not required\n",
    "    \n",
    "    \n",
    "    aggregated_vector = sum(nlp(word).vector/len(nlp(word).vector) for word in row) / len(row)\n",
    "    #print(aggregated_vector)\n",
    "    \n",
    "    #Reshape to 2-D\n",
    "    #word = word.reshape(1, -1)\n",
    "    aggregated_vector = aggregated_vector.reshape(1, -1)\n",
    "    \n",
    "    \n",
    "    # Calculate similarity between aggregated vector and each word vector\n",
    "    similarities = [cosine_similarity(aggregated_vector, nlp(word).vector.reshape(1,-1)) for word in row]\n",
    "    \n",
    "\n",
    "    # Select the word with the highest similarity\n",
    "    central_word_index = np.argmax(similarities)\n",
    "    return row[central_word_index]\n",
    "\n",
    "#cluster_df[\"keywords_glove_lemm\"]=cluster_df['tags_lemmatized'].apply(lambda row: central_word_cossim(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_word_gen(sbert_clusters):\n",
    "    #print(sbert_clusters)\n",
    "    dict_df={\"cluster\":sbert_clusters.keys(),\"tags\":sbert_clusters.values()}\n",
    "    tags_cluster=[]\n",
    "    for i in dict_df[\"tags\"]:\n",
    "        tags=[]\n",
    "        for j in i:\n",
    "            if \" \" in j:\n",
    "                j=j.split()\n",
    "                for k in j:\n",
    "                    tags.append(k)\n",
    "            else:\n",
    "                tags.append(j)\n",
    "        tags_cluster.append(tags)\n",
    "    cluster_df=pd.DataFrame.from_dict(dict_df)\n",
    "    cluster_df[\"keywords_glove\"]=cluster_df['tags'].apply(lambda row: central_word_cossim(row))\n",
    "    cluster_dict = {}\n",
    "    for i,data in cluster_df.iterrows():\n",
    "        cluster_dict[data['keywords_glove']] = data['tags']\n",
    "    #print(cluster_dict)\n",
    "    \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tags_to_clusters(tag_list):\n",
    "    clusters = []\n",
    "    for key, value in cluster_dict.items():\n",
    "        for tag in tag_list:\n",
    "            if tag in value:\n",
    "                clusters.append(key)\n",
    "        \n",
    "    #clusters = [key for key, value in sbert_clusters.items() if any(tag in value for tag in tag_list)]\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d24454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(\"capstone_data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 100\n",
    "combined_df = pd.read_csv(\"keybert_thresh_onlyen_0.1_nltk.csv\")\n",
    "combined_df['tags'] = combined_df['tags'].apply(ast.literal_eval)\n",
    "combined_df['tags_keybert'] = combined_df['tags_keybert'].apply(ast.literal_eval)\n",
    "combined_df['combined_tags'] = combined_df['tags'] + combined_df['tags_keybert']\n",
    "for index, row in combined_df.iterrows():\n",
    "    tags_list = row['tags'].copy()\n",
    "    tags_keybert_list = row['tags_keybert'].copy()\n",
    "    if len(tags_list) < 5:\n",
    "        for word in tags_keybert_list:\n",
    "            if word not in tags_list and len(tags_list)<5 and word.lower() in words.words():\n",
    "                tags_list.append(word)\n",
    "        combined_df.at[index, 'combined_tags'] = tags_list\n",
    "#combined_df['combined_tags'] = combined_df['tags'] + combined_df['tags_keybert']\n",
    "df_orig_loop = df_orig.copy()\n",
    "#df_orig_loop.to_csv(\"clusters_master_df.csv\")\n",
    "num = 1\n",
    "while num_clusters > 50:\n",
    "    sbert_clusters = cluster_creation(combined_df)\n",
    "    num_clusters = len(sbert_clusters.keys())\n",
    "    print(\"clusters created\")\n",
    "    print(num_clusters)\n",
    "    cluster_dict = root_word_gen(sbert_clusters)\n",
    "    print(\"generated root word\")\n",
    "    df_orig = combined_df.copy()\n",
    "    df_orig['tag_clusters'] = df_orig['combined_tags'].apply(map_tags_to_clusters)\n",
    "    print('tag clusters mapped')\n",
    "    df_orig.to_csv(\"clusters_master_df_\"+str(num)+\".csv\")\n",
    "    num+=1\n",
    "    combined_df = df_orig[[\"tag_clusters\"]]\n",
    "    combined_df = combined_df.rename(columns={\"tag_clusters\": \"combined_tags\"})\n",
    "    print('new df created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.read_csv(\"clusters_master_df_\"+str(1)+\".csv\")\n",
    "for i in range(1,num-1):\n",
    "    right = pd.read_csv(\"clusters_master_df_\"+str(i+1)+\".csv\")\n",
    "    df_cluster = left.merge(right, how = \"inner\", left_on=\"tag_clusters\", right_on=\"combined_tags\")\n",
    "    left = df_cluster.drop([\"tag_clusters_x\",\"combined_tags_y\"],axis=1)\n",
    "    left = left.rename({\"tag_clusters_y\":\"tag_clusters\"},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c193b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedea289",
   "metadata": {},
   "source": [
    "## Co-occurence of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.zeros((num_clusters, num_clusters), dtype=int)\n",
    "document_counts = np.zeros(num_clusters, dtype=int)\n",
    "clusters = list(cluster_dict.keys())\n",
    "left['tag_clusters'] = left['tag_clusters'].apply(ast.literal_eval)\n",
    "for tags in left['tag_clusters']:\n",
    "    #print(tags)\n",
    "    for tag in tags:\n",
    "        cluster_index = clusters.index(tag)\n",
    "        document_counts[cluster_index] += 1\n",
    "    \n",
    "    # Update the matrix based on the co-occurrence of clusters in each row\n",
    "    for i in range(len(tags)):\n",
    "        for j in range(i + 1, len(tags)):\n",
    "            # Find the indices of the clusters in the matrix\n",
    "            index_i = clusters.index(tags[i])\n",
    "            index_j = clusters.index(tags[j])\n",
    "            \n",
    "            if index_i == index_j:\n",
    "                continue\n",
    "\n",
    "            # Update the matrix\n",
    "            correlation_matrix[index_i, index_j] += 1\n",
    "            correlation_matrix[index_j, index_i] += 1\n",
    "\n",
    "# Create a DataFrame from the matrix\n",
    "correlation_df = pd.DataFrame(correlation_matrix, columns=clusters, index=clusters)\n",
    "\n",
    "# Normalize the correlation_df\n",
    "for i in range(len(correlation_df.index)):\n",
    "    for j in range(i + 1, len(correlation_df.columns)):\n",
    "        denominator = document_counts[i] + document_counts[j]\n",
    "        if denominator > 0:\n",
    "            correlation_df.iloc[i, j] /= denominator\n",
    "            correlation_df.iloc[j, i] /= denominator\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67282d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation_df, cmap=\"coolwarm\",annot=False, linewidths=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76655a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38ff01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5b955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96959ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
